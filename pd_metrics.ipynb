{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f38b7ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f38b7ce",
        "outputId": "8117ef2b-c57c-422c-dc68-a3a064f0ef5b"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import itertools\n",
        "import json\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from util.algorithms import MultivariateCalibrate, SynTF, TEM, TruncatedGumbel, VickreyMechanism, Mahalanobis, SanText\n",
        "from util.wordvec_load import LoadGlove, get_glove_embeddings, get_glove_embeddings_st\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e97a01c",
      "metadata": {
        "id": "2e97a01c"
      },
      "outputs": [],
      "source": [
        "glove_files = [\"/path/to/glove.6B.50d.txt\", \"/path/to/glove.6B.100d.txt\", \"/path/to/glove.6B.300d.txt\"] # INSERT PATH TO FILES (Download: https://nlp.stanford.edu/projects/glove/)\n",
        "dim_list = [50, 100, 300]\n",
        "dataset_types = [\"imdb\", \"ag_news\"]\n",
        "nclass_list = {\"imdb\": 2, \"ag_news\": 4}\n",
        "epsilons = [1, 5, 10]\n",
        "algo_types = [\"MultivariateCalibrate\", \"SynTF\", \"TEM\", \"Mahalanobis\", \"TruncatedGumbel\", \"VickreyMechanism\", \"SanText\"]\n",
        "max_min = {\n",
        "    \"imdb\": {\n",
        "        50: (15.329304595006077, 0.3513605074973775),\n",
        "        100: (14.607502691398507, 0.6062098738133386),\n",
        "        300: (18.082769927871894, 0.788992252232739)\n",
        "    },\n",
        "    \"ag_news\": {\n",
        "        50: (13.526640006274114, 0.3513605074973775),\n",
        "        100: (14.319089771129153, 0.6062098738133386),\n",
        "        300: (17.898080316889402, 0.788992252232739)\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d83172",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04d83172",
        "outputId": "5007bddf-b7e5-4271-95f6-ecf882fe5d65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l = [dataset_types, list(zip(glove_files, dim_list)), epsilons, [algo_types[-1]]]\n",
        "experiments = list(itertools.product(*l))\n",
        "len(experiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b021b70",
      "metadata": {
        "id": "7b021b70"
      },
      "outputs": [],
      "source": [
        "def plausible_deniability_metrics(obj, sampled_words):\n",
        "    Nws = []\n",
        "    Sws = []\n",
        "    eta = 0.01\n",
        "    for word in sampled_words:\n",
        "        same_as_word = 0\n",
        "        perturbed_words = list()\n",
        "        num_trials = 100\n",
        "        for i in range(num_trials):\n",
        "            w = obj.replace_word(word)\n",
        "            if w == word:\n",
        "                same_as_word += 1\n",
        "            else:\n",
        "                perturbed_words.append(w)\n",
        "        ctr = Counter(perturbed_words)\n",
        "        N_w = (same_as_word / num_trials) * 100\n",
        "        S_w = len(ctr)\n",
        "        Nws.append(N_w)\n",
        "        Sws.append(S_w)\n",
        "\n",
        "    return Nws, Sws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b3118d3",
      "metadata": {
        "id": "8b3118d3"
      },
      "outputs": [],
      "source": [
        "pd_scores = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c64700",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89c64700",
        "outputId": "948b659c-f57f-4055-db43-cdde4d76933a"
      },
      "outputs": [],
      "source": [
        "for e in experiments:\n",
        "    print(e)\n",
        "\n",
        "    if e[0] == \"imdb\":\n",
        "      SEED = 2759\n",
        "    elif e[0] == \"ag_news\":\n",
        "      SEED = 19\n",
        "\n",
        "    df_pre_train = pd.read_csv('Data/{}_preprocessed_train.csv'.format(e[0]))\n",
        "    X_train = df_pre_train['text'].values\n",
        "    y_train = df_pre_train['label'].values\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    vocab_size = len(tokenizer.word_index)+1\n",
        "\n",
        "    dim = e[1][1]\n",
        "\n",
        "    epsilon = e[2]\n",
        "    curr_algo = e[3]\n",
        "\n",
        "    wv_model = LoadGlove(e[1][0])\n",
        "    if curr_algo == algo_types[6]:\n",
        "      embedding_matrix, wordlist = get_glove_embeddings_st(embeddings_index=wv_model, dim=dim, tokenizer=tokenizer)\n",
        "    else:\n",
        "      embedding_matrix = get_glove_embeddings(embeddings_index=wv_model, dim=dim, tokenizer=tokenizer)\n",
        "\n",
        "    if curr_algo == algo_types[0]:\n",
        "        obj = MultivariateCalibrate(vocab_dict=tokenizer.word_index, epsilon=epsilon, embed_type=embed_type, wv_model=wv_model, embedding_matrix=embedding_matrix, dim=dim)\n",
        "\n",
        "    elif curr_algo == algo_types[1]:\n",
        "        sensitivity=1.0\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        obj = SynTF(epsilon=epsilon, sensitivity=sensitivity, vectorizer=vectorizer, data = X_train)\n",
        "\n",
        "    elif curr_algo == algo_types[2]:\n",
        "        obj = TEM(vocab_dict=tokenizer.word_index, epsilon=epsilon, embed_type=embed_type, wv_model=wv_model, embedding_matrix=embedding_matrix, dim=dim, vocab_size=vocab_size)\n",
        "\n",
        "    elif curr_algo == algo_types[3]:\n",
        "        lambd = 0.2\n",
        "        cov_mat = np.cov(embedding_matrix, rowvar=False)/np.var(embedding_matrix)\n",
        "        identity_mat = np.identity(dim)\n",
        "        obj = Mahalanobis(vocab_dict=tokenizer.word_index, epsilon=epsilon, embed_type=embed_type, wv_model=wv_model,\n",
        "                    embedding_matrix=embedding_matrix, cov_mat=cov_mat, identity_mat=identity_mat, lambd=lambd, dim=dim)\n",
        "\n",
        "    elif curr_algo == algo_types[4]:\n",
        "        obj = TruncatedGumbel(tokenizer=tokenizer,\n",
        "                            epsilon=epsilon,\n",
        "                            embed_type=embed_type,\n",
        "                            wv_model=wv_model,\n",
        "                            embedding_matrix=embedding_matrix,\n",
        "                            dim = dim,\n",
        "                            max_inter_dist=max_min[e[0]][dim][0],\n",
        "                            min_inter_dist=max_min[e[0]][dim][1])\n",
        "\n",
        "    elif curr_algo == algo_types[5]:\n",
        "        obj = VickreyMechanism(tokenizer = tokenizer,\n",
        "                            epsilon = epsilon,\n",
        "                            embed_type = embed_type,\n",
        "                            wv_model = wv_model,\n",
        "                            embedding_matrix = embedding_matrix,\n",
        "                            dim = dim,\n",
        "                            k = 2, t = [0.5, 0.5])\n",
        "    elif curr_algo == algo_types[6]:\n",
        "      obj = SanText(vocab_list=wordlist, epsilon=epsilon, embed_type=\"glove\", wv_model=wv_model, embedding_matrix=embedding_matrix, dim=dim)\n",
        "\n",
        "    temp = {}\n",
        "    word_list = list(tokenizer.word_index.keys())\n",
        "\n",
        "    random.seed(SEED)\n",
        "    sampled_words = random.sample(word_list, k = 25)\n",
        "    temp[\"sampled_words\"] = sampled_words\n",
        "    Nws, Sws = plausible_deniability_metrics(obj, sampled_words)\n",
        "    temp[\"n_w\"] = np.mean(Nws)\n",
        "    temp[\"s_w\"] = np.mean(Sws)\n",
        "    print(temp)\n",
        "    pd_scores[str(e)] = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0089fd42",
      "metadata": {
        "id": "0089fd42"
      },
      "outputs": [],
      "source": [
        "with open(\"pd_scores.json\", 'w') as out:\n",
        "    json.dump(pd_scores, out, indent=3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
