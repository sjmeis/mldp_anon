{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_fjSD2Nwo9g",
        "outputId": "1487b4a4-5dd1-484c-b772-fac8014c7fae"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.linalg import sqrtm\n",
        "from nltk.corpus import wordnet\n",
        "import os\n",
        "import json\n",
        "import nltk\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "\n",
        "from util.algorithms import MultivariateCalibrate, SynTF, TEM, Mahalanobis, TruncatedGumbel, VickreyMechanism, SanText\n",
        "from util.wordvec_load import LoadGlove, get_glove_embeddings, get_glove_embeddings_st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cu6lE7gNx27f"
      },
      "outputs": [],
      "source": [
        "glove_files = [\"/path/to/glove.6B.50d.txt\", \"/path/to/glove.6B.100d.txt\", \"/path/to/glove.6B.300d.txt\"] # INSERT PATH TO FILES (Download: https://nlp.stanford.edu/projects/glove/)\n",
        "dim_list = [50, 100, 300]\n",
        "dataset_types = [\"imdb\", \"ag_news\"]\n",
        "dataset_names = {\n",
        "    \"imdb\": ['Data/imdb_preprocessed_train.csv', 'Data/imdb_preprocessed_test.csv'],\n",
        "    \"ag_news\": ['Data/ag_news_preprocessed_train.csv', 'Data/ag_news_preprocessed_test.csv']\n",
        "}\n",
        "nclass_list = {\"imdb\": 2, \"ag_news\": 4}\n",
        "\n",
        "# precomputed values for Truncated Gumbel (to sped up init)\n",
        "max_min = {\n",
        "    \"imdb\": {\n",
        "        50: (15.329304595006077, 0.3513605074973775),\n",
        "        100: (14.607502691398507, 0.6062098738133386),\n",
        "        300: (18.082769927871894, 0.788992252232739)\n",
        "    },\n",
        "    \"ag_news\": {\n",
        "        50: (13.526640006274114, 0.3513605074973775),\n",
        "        100: (14.319089771129153, 0.6062098738133386),\n",
        "        300: (17.898080316889402, 0.788992252232739)\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmwRN-dtFxzy"
      },
      "outputs": [],
      "source": [
        "if Path(\"perturb.json\").is_file() == True:\n",
        "  with open(\"perturb.json\", 'r') as f:\n",
        "    results = json.load(f)\n",
        "else:\n",
        "  results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkwfXgaA6QgQ",
        "outputId": "f40aecb3-c369-47e1-d6b7-b5537be73879"
      },
      "outputs": [],
      "source": [
        "for task in dataset_types:\n",
        "  num_classes = nclass_list[task]\n",
        "\n",
        "  df_pre_train = pd.read_csv('Data/{}_preprocessed_train.csv'.format(task))\n",
        "  X_train = df_pre_train['text'].values\n",
        "  y_train = df_pre_train['label'].values\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(X_train)\n",
        "  vocab_size = len(tokenizer.word_index)+1\n",
        "\n",
        "  algo_types = [\"MultivariateCalibrate\", \"SynTF\", \"TEM\", \"Mahalanobis\", \"TruncatedGumbel\", \"VickreyMechanism\", \"SanText\"]\n",
        "  epsilons = [1.0, 5.0, 10.0]\n",
        "  l = [[task], list(zip(glove_files, dim_list)), algo_types, epsilons]\n",
        "  experiments = itertools.product(*l)\n",
        "\n",
        "  for e in experiments:\n",
        "    if str(e) in results:\n",
        "      continue\n",
        "\n",
        "    we_filename = e[1][0]\n",
        "    dim = e[1][1]\n",
        "\n",
        "    wv_model = LoadGlove(we_filename)\n",
        "    if curr_algo == algo_types[6]:\n",
        "        embedding_matrix, wordlist = get_glove_embeddings_st(embeddings_index=wv_model, dim=dim, tokenizer=tokenizer)\n",
        "    else:\n",
        "        embedding_matrix = get_glove_embeddings(embeddings_index=wv_model, dim=dim, tokenizer=tokenizer)\n",
        "\n",
        "    curr_algo = e[2]\n",
        "    epsilon = e[3]\n",
        "\n",
        "    if curr_algo == algo_types[0]:\n",
        "        obj = MultivariateCalibrate(vocab_dict=tokenizer.word_index, epsilon=epsilon, embed_type=\"glove\", wv_model=wv_model, embedding_matrix=embedding_matrix, dim=dim)\n",
        "    elif curr_algo == algo_types[1]:\n",
        "        sensitivity=1.0\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        obj = SynTF(epsilon=epsilon, sensitivity=sensitivity, vectorizer=vectorizer, data = X_train)\n",
        "\n",
        "    elif curr_algo == algo_types[2]:\n",
        "        obj = TEM(vocab_dict=tokenizer.word_index, epsilon=epsilon, embed_type=\"glove\", wv_model=wv_model, embedding_matrix=embedding_matrix, dim=dim, vocab_size=vocab_size)\n",
        "    elif curr_algo == algo_types[3]:\n",
        "        lambd = 0.2\n",
        "        cov_mat = np.cov(embedding_matrix, rowvar=False)/np.var(embedding_matrix)\n",
        "        identity_mat = np.identity(dim)\n",
        "        obj = Mahalanobis(vocab_dict=tokenizer.word_index, epsilon=epsilon, embed_type=\"glove\", wv_model=wv_model,\n",
        "                      embedding_matrix=embedding_matrix, cov_mat=cov_mat, identity_mat=identity_mat, lambd=lambd, dim=dim)\n",
        "    elif curr_algo == algo_types[4]:\n",
        "        obj = TruncatedGumbel(tokenizer=tokenizer,\n",
        "                          epsilon=epsilon,\n",
        "                          embed_type=\"glove\",\n",
        "                          wv_model=wv_model,\n",
        "                          embedding_matrix=embedding_matrix,\n",
        "                          dim = dim,\n",
        "                          max_inter_dist=max_min[e[0]][dim][0],\n",
        "                          min_inter_dist=max_min[e[0]][dim][1])\n",
        "    elif curr_algo == algo_types[5]:\n",
        "        obj = VickreyMechanism(tokenizer = tokenizer,\n",
        "                          epsilon = epsilon,\n",
        "                          embed_type = \"glove\",\n",
        "                          wv_model = wv_model,\n",
        "                          embedding_matrix = embedding_matrix,\n",
        "                          dim = dim,\n",
        "                          k = 2, t = [0.5, 0.5])\n",
        "    elif curr_algo == algo_types[6]:\n",
        "        # WARNING: SanText requires high amounts of RAM\n",
        "        obj = SanText(vocab_list=wordlist, \n",
        "                     epsilon=epsilon, \n",
        "                     embed_type=\"glove\", \n",
        "                     wv_model=wv_model, \n",
        "                     embedding_matrix=embedding_matrix, \n",
        "                     dim=dim)\n",
        "\n",
        "    print(e)\n",
        "    batch_size = 50\n",
        "\n",
        "    df_train_dp = pd.DataFrame(columns=['text', 'label'])\n",
        "    if Path(\"Data/perturbed\").is_dir() == False:\n",
        "       os.makedirs(\"Data/perturbed\")\n",
        "    save_filepath = os.path.join(\"Data/perturbed\", '{}_{}_perturbed_train_{}_{}.csv'.format(task, curr_algo, dim, epsilon)) \n",
        "    df_train_dp.to_csv(save_filepath, mode='w', index=False)\n",
        "\n",
        "    X=X_train\n",
        "    y=y_train\n",
        "\n",
        "    for i in tqdm(range(0, len(X), batch_size)):\n",
        "        sub_train_dp = [\" \".join([obj.replace_word(w) for w in sentence.split()]) for sentence in X[i:i+batch_size]]\n",
        "        df_train_dp = pd.DataFrame({'text': sub_train_dp, 'label': list(y[i:i+batch_size])}, columns=['text', 'label'])\n",
        "        df_train_dp.to_csv(save_filepath, mode='a', index=False, header=False)\n",
        "\n",
        "\n",
        "    results[str(e)] = {}\n",
        "    results[str(e)][\"saved\"] = save_filepath\n",
        "\n",
        "    with open(\"perturb.json\", 'w') as out:\n",
        "      json.dump(results, out, indent=3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
