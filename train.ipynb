{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7199,"status":"ok","timestamp":1687261615297,"user":{"displayName":"Stephen Meisenbacher","userId":"18259154938828106095"},"user_tz":-120},"id":"YKQaX5wVO8Uc"},"outputs":[],"source":["from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import json\n","import os\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from util.train import embed_train_model\n","from util.wordvec_load import LoadGlove, get_glove_embeddings"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":417,"status":"ok","timestamp":1687261619342,"user":{"displayName":"Stephen Meisenbacher","userId":"18259154938828106095"},"user_tz":-120},"id":"pSUxM72mPMCN"},"outputs":[],"source":["NUM_TRAIN = 5\n","nclass = {\"imdb\": 2, \"ag_news\": 4}"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1434,"status":"ok","timestamp":1687261628809,"user":{"displayName":"Stephen Meisenbacher","userId":"18259154938828106095"},"user_tz":-120},"id":"oSj7K1TEPOag"},"outputs":[],"source":["if Path(\"train.json\").is_file() == True:\n","  with open(\"train.json\", 'r') as f:\n","      train = json.load(f)\n","else:\n","  train = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":962086,"status":"ok","timestamp":1687262601078,"user":{"displayName":"Stephen Meisenbacher","userId":"18259154938828106095"},"user_tz":-120},"id":"Z_mOWEA1Pc-j","outputId":"00b61137-39d5-4cd7-858f-8701b6d7758a"},"outputs":[],"source":["for f in Path(\"Data/perturbed\").rglob(\"*.csv\"):\n","    dim = int(f.stem.split('_')[-2])\n","\n","    # CHANGE file paths for GloVe\n","    if dim == 50:\n","        glove_path = \"/path/to/glove.6B.50d.txt\"\n","    elif dim == 100:\n","        glove_path = \"/path/to/glove.6B.100d.txt\"\n","    elif dim == 300:\n","        glove_path = \"/path/to/glove.6B.300d.txt\"\n","\n","    df_train_dp = pd.read_csv(f)\n","    if \"imdb\" in f.name:\n","        df_pre_train = pd.read_csv(\"Data/imdb_preprocessed_train.csv\")\n","        df_pre_test = pd.read_csv(\"Data/imdb_preprocessed_test.csv\")\n","        num_classes = nclass[\"imdb\"]\n","    elif \"ag_news\" in f.name:\n","        df_pre_train = pd.read_csv(\"Data/ag_news_preprocessed_train.csv\")\n","        df_pre_test = pd.read_csv(\"Data/ag_news_preprocessed_test.csv\")\n","        num_classes = nclass[\"ag_news\"]\n","\n","    X_train = df_pre_train['text'].values\n","    y_train = df_pre_train['label'].values\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(X_train)\n","    vocab_size = len(tokenizer.word_index)+1\n","\n","    wv_model = LoadGlove(glove_path)\n","    embedding_matrix = get_glove_embeddings(embeddings_index=wv_model, dim=dim, tokenizer=tokenizer)\n","\n","    X_train_dp = df_train_dp['text'].values\n","    y_train_dp = df_train_dp['label'].values\n","\n","    X_test = df_pre_test['text'].values\n","    y_test = df_pre_test['label'].values\n","\n","    y_train = to_categorical(y_train_dp)\n","    y_test = to_categorical(y_test)\n","\n","    training_sequences = tokenizer.texts_to_sequences(X_train_dp)\n","    maxlen = 500\n","    training_padded = pad_sequences(training_sequences, maxlen=maxlen)\n","\n","    testing_sequences = tokenizer.texts_to_sequences(X_test)\n","    testing_padded = pad_sequences(testing_sequences, maxlen=maxlen)\n","\n","    X_train, X_test = training_padded, testing_padded\n","\n","    accuracies = []\n","    if Path(\"models/\").is_dir() == False:\n","        os.makedirs(\"models/\")\n","    model_savepath = \"models/\"\n","    for i in range(NUM_TRAIN):\n","        model, _ = embed_train_model(model_savepath, num_classes, embedding_matrix, X_train, y_train, X_test, y_test, vocab_size, maxlen, dim)\n","        _, accuracy = model.evaluate(testing_padded, y_test)\n","        accuracies.append(accuracy)\n","    train[f.stem] = np.mean(np.array(accuracies))\n","    print(\"{}: {}\".format(f.stem, np.mean(np.array(accuracies))))\n","\n","    with open(\"train.json\", 'w') as out:\n","        json.dump(train, out, indent=3)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN3OHdVp0mH66gcyJ/qNCE9","gpuType":"A100","machine_shape":"hm","mount_file_id":"1vg6UrJWJdeaaZ523jPDKbkf__nw4pchi","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
